---
title: "Initial Result and Code"
output: word_document
---
Note: So far this is what I have for my project, and things will be subject to change as I progress through the capstone and towards the final results and report.

Load the Dataset

```{r}
data <- read.csv('C:/Users/Ryan/Desktop/Capstone/listings.csv')
str(data)


```


Removing Redundant and not relevant variables for analysis e.g.(Repeated variables that represent the same thing)
```{r}
data$listing_url<-NULL # Not useful for analysis knowing the URL
data$scrape_id<-NULL # Not useful for analysis knowing the scrape ID
data$experiences_offered<-NULL # There is only one value it is none, since theres are all living spaces no experiences offered
data$thumbnail_url<-NULL #URL Links not useful for analysis and most are NA values
data$medium_url<-NULL #URL Links not useful for analysis and most are NA values
data$picture_url<-NULL #URL Links not useful for analysis
data$xl_picture_url<-NULL #URL Links not useful for analysis and most are NA values
data$thumbnail_url<-NULL #URL Links not useful for analysis
data$medium_url<-NULL #URL Links not useful for analysis
data$picture_url<-NULL #URL Links not useful for analysis
data$xl_picture_url<-NULL #URL Links not useful for analysis
data$host_url<-NULL #URL Links not useful for analysis
data$host_thumbnail_url<-NULL #URL Links not useful for analysis
data$host_picture_url<-NULL #URL Links not useful for analysis
data$host_name<-NULL #Already have ID , not need two have both 
data$country_code<-NULL # We already know it's in the US and in LA metro area
data$country<-NULL# We already know it's in the US
data$state<-NULL #We already know it's in CA since it's in LA metro area
data$host_neighbourhood<-NULL # The location of the host maybe not be useful
data$host_location<-NULL
data$host_has_profile_pic<-NULL
data$neighbourhood<-NULL #There is already a cleansed version of the neighbourhood, so this should be more accurate to go off of
data$is_location_exact<- NULL # I believe that using the general location of the long and lat shuold be sufficient for analysis


#These are redundant numbers that are already shown by minimum_nights and maximum_nights column 
data$maximum_maximum_nights<-NULL
data$maximum_minimum_nights<-NULL
data$maximum_nights_avg_ntm<-NULL
data$minimum_maximum_nights<-NULL
data$minimum_minimum_nights<-NULL
data$minimum_nights_avg_ntm<-NULL




data$market<-NULL # Already shown by City/Neighbourhod,it's going to be in LA metropolitan area,thus redundant information
data$smart_location<-NULL # Already shown by City/Neighbourhood
data$host_listings_count<-NULL # Doubled, there is already a total listings count


data$jurisdiction_names<-NULL #We already know that it is within the LA metro area
data$number_of_reviews_ltm<-NULL # Not really useful to know the amount last month, more interested in total
data$calculated_host_listings_count<-NULL #Repeated total listing counts for each host

data$license<-NULL # License number/ID not important for the analysis
data$is_business_travel_ready<-NULL #When checking the structure of the dataset, only one factor and it's all false
```



Fixing the Data types of variables
```{r}
str(data)
#These should be a character data type
data$name<-as.character(data$name)
data$summary<-as.character(data$summary)
data$space<-as.character(data$space)
data$description<-as.character(data$description)
data$neighborhood_overview<-as.character(data$neighborhood_overview)
data$notes<-as.character(data$notes)
data$transit<-as.character(data$transit)
data$access<-as.character(data$access)
data$interaction<-as.character(data$interaction)
data$house_rules<-as.character(data$house_rules)
data$host_since<-as.Date(data$host_since)
data$host_about<-as.character(data$host_about)

#Was not numeric
data$host_response_rate<-gsub('[%]','',data$host_response_rate)
data$host_response_rate<-as.numeric(data$host_response_rate) 
data$host_acceptance_rate<-gsub('[%]','',data$host_acceptance_rate)
data$host_acceptance_rate<-as.numeric(data$host_acceptance_rate) 

#Prices was not numeric
data$price<-gsub('[$]','',data$price)
data$price<-as.numeric(data$price)
data$weekly_price<-gsub('[$]','',data$weekly_price)
data$weekly_price<-as.numeric(data$weekly_price)
data$monthly_price<-gsub('[$]','',data$monthly_price)
data$monthly_price<-as.numeric(data$monthly_price)
data$security_deposit<-gsub('[$]','',data$security_deposit)
data$security_deposit<-as.numeric(data$security_deposit)
data$cleaning_fee<-gsub('[$]','',data$cleaning_fee)
data$cleaning_fee<-as.numeric(data$cleaning_fee)
data$extra_people<-gsub('[$]','',data$extra_people)
data$extra_people<-as.numeric(data$extra_people)

#After looking closely at the weekly price and monthly price , the amount don't actually checkout if you multiply the per night
#By 7 and there are many missing NA values for them as well.

length(which(is.na(data$monthly_price)==TRUE))
length(which(is.na(data$weekly_price)==TRUE))
data$weekly_price<-NULL
data$monthly_price<-NULL
#Better to remove the weekly/monthly prices due to the number of missing values, moreover AirBnb prices per night anyways


#Changing dates to datatype
data$last_scraped<-as.Date(data$last_scraped)
data$calendar_last_scraped<-as.Date(data$calendar_last_scraped)
data$first_review<-as.Date(data$first_review)
data$last_review<-as.Date(data$last_review)


#Cleaning amenities

data$amenities<-as.character(data$amenities)
data$amenities<-strsplit(data$amenities,",")
data$amenities<-gsub("[{}]","",data$amenities)
data$amenities<-gsub('["\"]',"",data$amenities)




```






Visualiztions of variables (histogram and boxplots)
```{r}

#Histograms
hist(data$host_response_rate)
hist(data$host_acceptance_rate)
hist(data$price)
hist(data$bedrooms)
hist(data$bathrooms)
hist(data$beds)
hist(data$security_deposit)
hist(data$cleaning_fee)

hist(data$availability_365)
hist(data$review_scores_rating)


#Boxplots
boxplot(data$host_response_rate)
boxplot(data$host_acceptance_rate)
boxplot(data$price)
boxplot(data$bedrooms)
boxplot(data$bathrooms)
boxplot(data$beds)
boxplot(data$security_deposit)
boxplot(data$cleaning_fee)

boxplot(data$availability_365)
boxplot(data$review_scores_rating)
boxplot(data$review_scores_rating)
```



Dealing with the outliers in the dataset
```{r}
#Decided to replace the outlier values with the median values, if I used mean instead extreme outliers could possibly affect the mean

boxplot.stats(data$price)
data$price[which(data$price>334)]<-median(data$price,na.rm=TRUE) #334 is the uppper whisker, no values that are below the lower whisker

boxplot.stats(data$host_acceptance_rate)
data$host_acceptance_rate[which(data$host_acceptance_rate<63)]<-median(data$host_acceptance_rate,na.rm=TRUE) 

boxplot.stats(data$bedrooms)
data$bedrooms[which(data$bedrooms>3)]<-median(data$bedrooms,na.rm=TRUE)#3 is the upper whisker, no values that are below the lower whisker

boxplot.stats(data$bathrooms)
data$bathrooms[which(data$bathrooms>3.5)]<-median(data$bathrooms,na.rm=TRUE)#3.5 is the upper whisker, no values that are below the lower whisker

boxplot.stats(data$beds)
data$beds[which(data$beds>3)]<-median(data$beds,na.rm=TRUE) #3 is the upper whisker, no values that are below the lower whisker

boxplot.stats(data$security_deposit)
data$security_deposit[which(data$security_deposit>750)]<-median(data$security_deposit,na.rm=TRUE) # 750 is the upper whisker, no values that are below the lower whisker

boxplot.stats(data$cleaning_fee)
data$cleaning_fee[which(data$cleaning_fee>265)]<-median(data$cleaning_fee,na.rm=TRUE) # 265 is the upper whisker, no values that are below the lower whisker

boxplot.stats(data$availability_365) #No outliers

boxplot.stats(data$review_scores_rating) 
data$review_scores_rating[which(data$review_scores_rating<83)]<-median(data$review_scores_rating,na.rm=TRUE) # 83 is the lower whisker, no values above the upper whisker


```






Correlation of the numerical attributes in the dataset
```{r}
#First create an numerical attribute dataframe of the dataset
#Then create a correlation matrix
#Remove any highly correlated attributes to one another

num_cols_only <- unlist(lapply(data,is.numeric)) #When you lapply is.numeric , it returns a list therefore you need to unlist to use it to make a numeric dataframe
data_numeric<-data[,num_cols_only]
cor_matrix<-cor(data_numeric,use="complete.obs")
library(corrplot)
corrplot(cor_matrix,type="upper")

#Bedrooms and bathrooms is moderately postively correlated, we can remove bathrooms and keep bedrooms
#Availability 365 is moderately postively correlated with the other availability values which makes sense
#Accomadates number is moderately postively correlated with bedrooms,which makes sense as if you have more rooms you can accomodate more people
```



Dealing with NA values or missing values in the dataset
```{r}



#Now that the outliers have been replaced, I will want to replace any NA values with the mean value
#If there are records with missing values that are non numerical, I will remove that record if it is logical to do so

data$price[which(is.na(data$price)==TRUE)]<-mean(data$price,na.rm=TRUE)
data$bedrooms[which(is.na(data$bedrooms)==TRUE)]<-mean(data$bedrooms,na.rm=TRUE)
data$bathrooms[which(is.na(data$bathrooms)==TRUE)]<-mean(data$bathrooms,na.rm=TRUE)
data$beds[which(is.na(data$beds)==TRUE)]<-mean(data$beds,na.rm=TRUE)
data$security_deposit[which(is.na(data$security_deposit)==TRUE)]<-mean(data$security_deposit,na.rm=TRUE)
data$review_scores_rating[which(is.na(data$review_scores_rating)==TRUE)]<-mean(data$review_scores_rating,na.rm=TRUE)
data$review_scores_accuracy[which(is.na(data$review_scores_accuracy)==TRUE)]<-mean(data$review_scores_accuracy,na.rm=TRUE)
data$review_scores_checkin[which(is.na(data$review_scores_checkin)==TRUE)]<-mean(data$review_scores_checkin,na.rm=TRUE)
data$review_scores_cleanliness[which(is.na(data$review_scores_cleanliness)==TRUE)]<-mean(data$review_scores_cleanliness,na.rm=TRUE)
data$review_scores_value[which(is.na(data$review_scores_value)==TRUE)]<-mean(data$review_scores_value,na.rm=TRUE)
data$review_scores_location[which(is.na(data$review_scores_location)==TRUE)]<-mean(data$review_scores_location,na.rm=TRUE)
data$review_scores_communication[which(is.na(data$review_scores_communication)==TRUE)]<-mean(data$review_scores_communication,na.rm=TRUE)
data$cleaning_fee[which(is.na(data$cleaning_fee)==TRUE)]<-mean(data$cleaning_fee,na.rm=TRUE)

data<-data[which(!data$host_is_superhost==""),] # Only 3 values that were blank in the dataset, removed
data<-data[which(!data$bed_type==""),] #10 blank values removed
data<-data[which(!data$host_response_time==""),] # Only 3 values that were blank in the dataset, which also happens to be the same rows of the superhost that were blank
data<-data[which(!data$price==0),]#I also removed prices that were at 0, since it doesn't make sense for a listing to be at 0 for the context of the problem,since if it is 0 then that means it's free AirBnb.

#After removing the blank values in the dataset, there was still blank factors in the str(data)
str(data)

#Removed for analysis
levels(data$host_is_superhost)[which(levels(data$host_is_superhost)=="")]<-NA 
levels(data$bed_type)[which(levels(data$bed_type)=="")]<-NA 
levels(data$host_response_time)[which(levels(data$host_response_time)=="")]<-NA
levels(data$host_identity_verified)[which(levels(data$host_identity_verified)=="")]<-NA 

```






Backward elimination and checking assumptions of the linear regression model
```{r}

#Still trying to figure out the best way incorporate amenities into my models,and still a work in progress for the final results/report
model<-lm(price~bedrooms+property_type+beds+host_is_superhost+neighbourhood_cleansed+cleaning_fee+security_deposit+
            review_scores_rating+bed_type+review_scores_accuracy+review_scores_checkin+review_scores_cleanliness+
            review_scores_communication+review_scores_location+review_scores_value,data=data)

library(MASS)

summary(model)
stepAIC(model,direction="backward") #Through backward elimination, we eliminated the attribute review_score_accuracy and review_score_communication


model<-lm(price~bedrooms+property_type+beds+host_is_superhost+neighbourhood_cleansed+cleaning_fee+security_deposit+
            review_scores_rating+bed_type+review_scores_checkin+review_scores_cleanliness
          +review_scores_location+review_scores_value,data=data) # removed due to backward regression

par(mfrow=c(2,2))#Easier to look at the plots in a 2x2 window
plot(model)

#Based on the plots, it looks like there isn't constant variance, but the other assumptions are upheld 
#To address this I transformed the dependent variable by taking the natural log of the price


model<-lm(log(price)~bedrooms+property_type+beds+host_is_superhost+neighbourhood_cleansed+cleaning_fee+security_deposit+
            review_scores_rating+bed_type+review_scores_checkin+review_scores_cleanliness
          +review_scores_location+review_scores_value,data=data)


plot(model)
```


In addition, I created a variation to the above model, since the context of the problem is to help new listers determine their price per night, it wouldn't make sense for them to have a rating or superhost on AirBnb.

```{r}
model2<-lm(price~bedrooms+property_type+beds+neighbourhood_cleansed+cleaning_fee+security_deposit+bed_type,data=data)
par(mfrow=c(2,2))#Easier to look at the plots in a 2x2 window
plot(model2)

#Again, need to log the price as there isn't constant variance and for a better model

model2<-lm(log(price)~bedrooms+property_type+beds+neighbourhood_cleansed+cleaning_fee+security_deposit+bed_type,data=data)
plot(model2)
```


Spliting the dataset into a training set and test set using 10 cross fold validation. Then create the above model using the 10 fold cross validation. The benefit of 10 cross fold validation, is that it can use the entire dataset to train the model on.

It seems that the dataset provided by insideairbnb website doesn't show when they host actually put up their listing, but provides when they scraped the listing off of the website. I also checked actual AirBnb website as well,there's no way to find out the actual time they put up the listing.However, I do find it important to know when they scraped off the listing, since it means that, that was the price per night of their listing on the date of the scraping, which means that the host still thought that their property was worth that much at the time of the scrap and market conditions are still represented at the time of the scraping.


```{r}

#Looking at the dataset of when the data is scraped,it is all in 2020 with a few days difference for some of the listings
library(caret)

train_cv<-trainControl(method="cv",number=10) #10 cross fold validation

model_cv<-train(log(price)~bedrooms+property_type+beds+host_is_superhost+neighbourhood_cleansed+cleaning_fee+security_deposit+
            review_scores_rating+bed_type+review_scores_checkin+review_scores_cleanliness
          +review_scores_location+review_scores_value,data=data,trControl=train_cv,method="lm")


print(model_cv)

#By printing our model, we have three metrics we can look at for our model, RMSE,R2,and MAE.We can look at these measures


#I looked into the warning message, which may be the result of colinearity of the independent variables, however I already looked into  numberic variables in the model and I didn't find anything of the sort.Thus, I think it MAY be an false warning. 



#The model variation we did above, I will do again for the 10 cross fold validation

model2_cv<-train(log(price)~bedrooms+property_type+beds+neighbourhood_cleansed+cleaning_fee+security_deposit+bed_type,data=data,trControl=train_cv,method="lm")

print(model2_cv)
```









Multinomial Logistic Model using 10 Fold Cross Validation
```{r}
#First split the price into categories such as low , medium , high.I think this is important to look at since when it comes to the tourism or hospitality industry, as a consumer myself I look at the price as a way to determine the luxary of an accomadation.Higher the price the more luxurious it is. I'm thinking of renaming these categories to other categories that will fit the context of the problem better, these could be temporary names.


range(data$price)



#10 is the loweest , 334 is the highest price , I want to split it between low, medium and high
#Thus, lowest I will set it as 10-118 as low inclusively, 119- 226 inclusively as medium, and 227-334 inclusively as high.

data$price_level<-NA

data$price_level[which(data$price<=118)]<-"Low"
data$price_level[which(data$price>=119)]<-"Medium"
data$price_level[which(data$price>=227)]<-"High"


data$price_level<-as.factor(data$price_level)
```


```{r}



train_cv<-trainControl(method="cv",number=10)

model_log_cv<-train(price_level~bedrooms+property_type+beds+host_is_superhost+neighbourhood_cleansed+cleaning_fee+security_deposit+
            review_scores_rating+bed_type+review_scores_checkin+review_scores_cleanliness
          +review_scores_location+review_scores_value,data=data,trControl=train_cv,method="multinom")

print(model_log_cv)


#Using a 10 fold cross validation and multinomial logistic regression takes a long time to train the model. 

#Again we have some metrics to look at for our multinomial regression model. 
```




I made a confusion matrix here to see more metrics of the multinomial logistic regression.

```{r}

log_predictions <-predict(model_log_cv,data)
actual_values <-data$price_level


confusionMatrix(log_predictions,actual_values)
```


Multinomial Logistic Regression Model Variation 



```{r}
model2_log_cv<-train(price_level~bedrooms+property_type+beds+neighbourhood_cleansed+cleaning_fee+security_deposit+bed_type,data=data,trControl=train_cv,method="multinom")

print(model2_log_cv)


log2_prediction<-predict(model2_log_cv,data)
confusionMatrix(log2_prediction,actual_values) 
```


 